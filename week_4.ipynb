{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.cross_validation import train_test_split as tts\n",
    "from sklearn import preprocessing as pr\n",
    "from sklearn.feature_extraction import DictVectorizer as DV\n",
    "from sklearn import metrics\n",
    "from sklearn import linear_model\n",
    "from sklearn import cross_validation\n",
    "from sklearn import ensemble\n",
    "import category_encoders as ce\n",
    "import warnings\n",
    "import xgboost as xgb\n",
    "warnings.simplefilter('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def get_numerical_features(sdata):\n",
    "    features = set(sdata.columns) & set([(\"Var%d\" % i) for i in range(1, 191)])\n",
    "    return list(features)\n",
    "\n",
    "\n",
    "def get_categorial_features(sdata):\n",
    "    features = set(sdata.columns) & set([(\"Var%d\" % i) for i in range(191, 231)])\n",
    "    return list(features)\n",
    "\n",
    "\n",
    "def get_kaggle_data():\n",
    "    # считываем данные\n",
    "    churn_train_data = pd.read_csv('churn_train_data.csv')\n",
    "    labels = churn_train_data['labels']\n",
    "    churn_train_data = churn_train_data.drop(['labels', 'ID'], axis=1)\n",
    "    churn_test_data = pd.read_csv('churn_test_data.csv')\n",
    "    churn_test_data = churn_test_data.drop(['ID'], axis=1)\n",
    "    return churn_train_data, labels, churn_test_data\n",
    "\n",
    "\n",
    "def write_ans(file_name, values):\n",
    "    # функция записи ответа\n",
    "    with open(file_name, 'w') as il:\n",
    "        il.write('ID,result\\n')\n",
    "        for (idx, element) in enumerate(values):\n",
    "            il.write(str(idx))\n",
    "            il.write(str(\",\"))\n",
    "            il.write(str(round(element, 10)))\n",
    "            il.write('\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def process_numerical_features(sdata, ndata):\n",
    "    features = get_numerical_features(sdata)\n",
    "    to_remove = set()\n",
    "    for (idx, feature) in enumerate(features):\n",
    "        non_na = np.array(sdata[feature].dropna())\n",
    "        mean, std = non_na.mean(), non_na.std()\n",
    "        if np.isnan(mean / std) or np.isinf(mean / std):\n",
    "            to_remove.add(feature)\n",
    "            continue\n",
    "\n",
    "        sdata[feature].fillna(mean, inplace=True)\n",
    "        ndata[feature].fillna(mean, inplace=True)\n",
    "    features = get_numerical_features(sdata)    \n",
    "    features = list(set(features) - to_remove)\n",
    "    return sdata[features].as_matrix(), ndata[features].as_matrix()\n",
    "\n",
    "\n",
    "def remove_incomplete_features(sdata, ndata, features):\n",
    "    to_remove = set()\n",
    "    for feature in features:\n",
    "        nan_part = sum(sdata[feature].isnull()) / float(sdata.shape[0])\n",
    "        if nan_part > 0.9:\n",
    "            sdata = sdata.drop(feature, axis=1)\n",
    "            ndata = ndata.drop(feature, axis=1)\n",
    "            to_remove.add(feature)    \n",
    "    return sdata, ndata\n",
    "\n",
    "def apply_str(data):\n",
    "    features = get_categorial_features(data)\n",
    "    for feature in features:\n",
    "        data[feature] = data[feature].apply(lambda x: str(x))\n",
    "    return data[features]\n",
    "\n",
    "\n",
    "def separate_categorial_features(sdata, ndata):\n",
    "    features = get_categorial_features(sdata)\n",
    "    first = set(filter(lambda x: len(set(sdata[x].values)) <= 15, features))\n",
    "    second = set(features) - set(first)\n",
    "    first, second = list(first), list(second)\n",
    "    return sdata[first], sdata[second], ndata[first], ndata[second]\n",
    "\n",
    "\n",
    "def one_hot(sdata, ndata):\n",
    "    encoder = DV(sparse = False)\n",
    "    sdata = encoder.fit_transform(sdata.T.to_dict().values())\n",
    "    ndata = encoder.transform(ndata.T.to_dict().values())\n",
    "    return sdata, ndata\n",
    "\n",
    "\n",
    "def label_encoder(sdata, ndata):\n",
    "    features = get_categorial_features(sdata)\n",
    "    for feature in features:\n",
    "        # применяем LabelEncoder на все строки\n",
    "        encoder = pr.LabelEncoder()\n",
    "        values = set(sdata[feature].values) | set(ndata[feature].values)\n",
    "        encoder.fit(list(values))\n",
    "        sdata[feature] = encoder.transform(sdata[feature])\n",
    "        ndata[feature] = encoder.transform(ndata[feature])\n",
    "    return sdata.as_matrix(), ndata.as_matrix()\n",
    "\n",
    "\n",
    "def process_categorial_features(sdata, ndata):\n",
    "    sdata, ndata = apply_str(sdata), apply_str(ndata)\n",
    "    features = get_categorial_features(sdata)\n",
    "    for feature in features:\n",
    "        if len(set(sdata[feature].values)) == 1:\n",
    "            # удаляем постоянные признаки\n",
    "            sdata = sdata.drop(feature, axis=1)\n",
    "            ndata = ndata.drop(feature, axis=1)\n",
    "    return label_encoder(sdata, ndata)\n",
    "\n",
    "\n",
    "def prepare_train_data(sdata, ndata, labels, truncate=False):\n",
    "    # удаляем фичи в которых большее количество пустых ячеек\n",
    "    sdata, ndata = remove_incomplete_features(sdata, ndata, list(sdata.columns))\n",
    "    # обрабатываем численные фичи\n",
    "    numerical_train_data, numerical_test_data = process_numerical_features(\n",
    "        sdata, ndata)\n",
    "    # обрабатываем категориальные признаки\n",
    "    categorial_train_data, categorial_test_data = process_categorial_features(\n",
    "        sdata, ndata)\n",
    "    prepared_train_data = np.concatenate([\n",
    "            numerical_train_data, categorial_train_data], axis=1)\n",
    "    prepared_test_data = np.concatenate([\n",
    "            numerical_test_data, categorial_test_data], axis=1)   \n",
    "    return prepared_train_data, np.array(labels), prepared_test_data\n",
    "\n",
    "\n",
    "churn_train_data, labels, churn_test_data = get_kaggle_data()\n",
    "X, y, ansX = prepare_train_data(churn_train_data, churn_test_data, labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.738165255516\n"
     ]
    }
   ],
   "source": [
    "def XGB(X, y, X_test, tree_size = 50, depth = 3, child_weight = 10):\n",
    "    estimator = xgb.XGBClassifier(\n",
    "        learning_rate=0.1, max_depth = depth, seed = 42, n_estimators = tree_size, min_child_weight = child_weight)\n",
    "    folds = cross_validation.cross_val_score(\n",
    "        estimator, X, y, cv = 5, scoring='roc_auc')\n",
    "    print(folds.mean())\n",
    "    estimator.fit(X, y)\n",
    "    return estimator.predict_proba(X_test)[:,1]\n",
    "\n",
    "values = XGB(X, y, ansX, tree_size = 100)\n",
    "write_ans('ans15.csv', values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [conda root]",
   "language": "python",
   "name": "conda-root-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
